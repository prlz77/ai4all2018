{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI4ALL Sessió 3\n",
    "\n",
    "Llibreries que cal instal·lar (amb conda o pip):\n",
    "* tqdm\n",
    "* unidecode\n",
    "\n",
    "## 1. Xarxes neurals recurrents\n",
    "Els objectius d'aquest exemple són:\n",
    "1. Aprendre sobre el funcionament de les xarxes neurals recurrents.\n",
    "2. Aprendre a utilitzar xarxes pre-entrenades\n",
    "\n",
    "### 1.1. Dades\n",
    "En aquest exemple aprendrem una xarxa neural recurrent a predir la següent lletra d'un text a partir de les anteriors. Per exemple:\n",
    "\n",
    "<img src=\"char_rnn/charseq.jpeg\" style=\"width: 50%;\"/>\n",
    "\n",
    "Com podeu veure, utilitzant la paraula \"hello\", per a la lletra \"h\" volem predir \"e\". Per a la lletra \"e\" volem predir \"l\", i així successivament. Com que les xarxes neurals recurrents tenen memòria, quan predigui la lletra \"o\", ho farà sabent que anteriorment s'ha entrat \"hell\".\n",
    "\n",
    "D'aquesta manera la xarxa apren com funciona la interacció entre lletres per a formar paraules, i és capaç d'utilitzar tot el què ha vist anteriorment per a predir la següent lletra.\n",
    "\n",
    "\n",
    "### 1.2. Xarxa\n",
    "L'ideal seria que la xarxa aprenguès a predir la següent lletra enrecordant-se de tot el que s'ha dit abans, **no obstant** els ordinadors no tenen capacitat suficient per a guardar un historial tan gran, i el que fem és definir una finestra de N lletres que anirà lliscant sobre el text. És a dir, la xarxa predirà la següent lletra enrecordant-se de les N lletres anteriors.\n",
    "\n",
    "Per a aquest exemple he pre-entrenat una xarxa recurrent de dues capes i 256 neurones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Recurrent(nn.Module):\n",
    "    def __init__(self, mida_alfabet, n_neurones, n_capes):\n",
    "        super().__init__()\n",
    "        self.n_neurones = n_neurones\n",
    "        self.n_capes = n_capes\n",
    "        self.mida_alfabet = mida_alfabet\n",
    "        # GRU es un tipus de xarxa neural recurrent més efectiu que el model basic (RNN)\n",
    "        self.rnn = nn.GRU(n_neurones, n_neurones, n_capes, batch_first=True, dropout=0.3)\n",
    "        # Utilitzem una capa de neurones per a convertir una lletra a un vector i vice-versa. Ex. a = (0.4, 0.2, 0.223)\n",
    "        self.linear_output = nn.Linear(n_neurones, mida_alfabet, bias=False)\n",
    "          \n",
    "    def forward(self, x, alfabet, genera_n=64, temperatura=1.0):\n",
    "        # Primer passem a la xarxa el text inicial\n",
    "        b, s = x.size()\n",
    "        y = nn.functional.embedding(x, self.linear_output.weight) # convertim lletres a vectors\n",
    "        estat = torch.zeros(self.n_capes, 1, self.n_neurones) # inicialitzem l'estat de la xarxa recurrent a zero (no ha vist cap lletra abans)\n",
    "        out, estat = self.rnn(y, estat) # passem estat inicial i les seqüencies de lletres a la xarxa recurrent\n",
    "        \n",
    "        # Utilitzem última lletra com a entrada per a predir la següent lletra. Repetim múltiples cops.\n",
    "        ret = []\n",
    "        for i in range(genera_n):\n",
    "            # Primer convertim a caracter de l'alfabet l'última predicció i l'afegim al text de sortida\n",
    "            out = out[0,-1,:].contiguous().view(1, self.n_neurones) # agafem última lletra i redimensionem\n",
    "            out = self.linear_output(out).view(1, 1, self.mida_alfabet) # convertim la sortida de vector a probabilitat de ser certa lletra\n",
    "            probabilitats = nn.functional.softmax(out * temperatura, dim=2).numpy().ravel() \n",
    "            lletra = alfabet[np.random.choice(np.arange(len(alfabet)), p=probabilitats)] # fem una ruleta sobre les mes provables\n",
    "            ret.append(lletra) # afegim la predicció actual a la sortida\n",
    "            \n",
    "            # Convertim l'última lletra del text a posició dins de l'alfabet\n",
    "            lletra_num = torch.LongTensor([alfabet.index(ret[-1])]).view(1, 1)\n",
    "            # Convertim de número a vector\n",
    "            y = nn.functional.embedding(lletra_num, self.linear_output.weight)\n",
    "            # Passem el vector que representa la sortida anterior com a entrada de la xarxa\n",
    "            out, estat = self.rnn(y, estat) # y = lletra, estat = estat anterior\n",
    "     \n",
    "            \n",
    "        return \"\".join(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carreguem dades de xarxa pre-entrenada**\n",
    "En aquest cas l'he entrenat a predir el text de \"Don Quijote\". Podeu trobar el fitxer d'entrenament a [https://github.com/prlz77/ai4all2018/sessio3/char_rnn](https://github.com/prlz77/ai4all2018/sessio3/char_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "carregat = torch.load(\"./char_rnn/model1.pth\", map_location=\"cpu\") # llegim fitxer guardat.\n",
    "\"\"\" En el fitxer tambe hi he guardat la correspondencia entre les lletres \n",
    "    acceptades i un nombre enter: a = 1, b = 2, etc. Això és necessari \n",
    "    perquè la xarxa treballa amb números.\n",
    "\"\"\"\n",
    "alfabet = carregat[\"alphabet\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara carreguem els pesos pre-entrenats a una xarxa recent creada. **L'arquitectura de la xarxa carregada i el seu recipient han de coincidir.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Recurrent(len(alfabet), 256, 2) # Creem xarxa inicial sense entrenar\n",
    "rnn.load_state_dict(carregat[\"net\"]) # AQUI ES ON CARREGUEM XARXA PRE-ENTRENADA\n",
    "rnn = rnn.cpu() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Predicció\n",
    "Finalment podem utilitzar la xarxa apresa amb *Don Quijote* per a generar text. Com? En comptes de donar nosaltres tota l'estona les lletres a l'entrada de la xarxa, li donem una petita entrada inicial i **fem que la xarxa utilitzi la seva propia sortida com a entrada.**\n",
    "\n",
    "Primer definim unes funcions auxiliars per a convertir el format les entrades i sortides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from char_rnn.entrena_rnn import preprocess\n",
    "import numpy as np\n",
    "\n",
    "# Mida_seq és la mida màxima que la nostra xarxa mira enrere\n",
    "def text_a_xarxa(text, alfabet):\n",
    "    text = preprocess(text)\n",
    "    return torch.LongTensor(list(map(lambda x: alfabet.index(x), text))).view(1, len(text))\n",
    "\n",
    "def xarxa_a_text(sortida, alfabet, temperatura=1.0):\n",
    "    probabilitats = nn.functional.softmax(sortida * temperatura, dim=2).numpy()\n",
    "    b, s, p = probabilitats.shape\n",
    "    probabilitats = probabilitats.reshape((b*s, p))\n",
    "    return \"\".join([alfabet[np.random.choice(np.arange(len(alfabet)), p=x)] for x in probabilitats])\n",
    "\n",
    "\n",
    "def prediu_text(entrada, quant=300, temperatura=1):\n",
    "    rnn.eval()\n",
    "    with torch.no_grad():\n",
    "        entrada_xarxa = text_a_xarxa(entrada, alfabet)\n",
    "        return rnn(entrada_xarxa, alfabet, genera_n=quant, temperatura=temperatura)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passem un text inicial a la xarxa, i la fem continuar des d'allà."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sancho, dijo don quijote. Te gusta el vino del mal se le habeis traido a todos los caballeros andantes y amparen y cardenio y como a su amo la proceder de la guerra en las cosas de la venta, porque le tocaba a los de pensamientos y se cuentan a la porfia, con todo esto, sancho, porque no se de la tal vez se ha de ser que el despecho de contento de memoria de las cosas que de la procura la cabeza a la mano antes que de la caballeria en la hermosa doncella de su amo, de quien el paso en este caballero de la compania de caballero, mas sabra el rico caballero atado de la mancha, que este es la que habia de ser mejor decir que se entrara \n"
     ]
    }
   ],
   "source": [
    "text_inicial = \"Sancho, dijo don quijote. Te gusta el vin\"\n",
    "print(text_inicial + prediu_text(text_inicial, quant=600, temperatura=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercicis\n",
    "1. Canvieu el text inicial i observeu els canvis a la sortida.\n",
    "2. Quin és l'efecte de la temperatura?\n",
    "3. Què passa si executeu la predicció múltiples cops? Per què?\n",
    "3. Quin és l'efecte del paràmetre quant?\n",
    "4. Canvieu el model per model2.pth. Quins canvis observeu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
